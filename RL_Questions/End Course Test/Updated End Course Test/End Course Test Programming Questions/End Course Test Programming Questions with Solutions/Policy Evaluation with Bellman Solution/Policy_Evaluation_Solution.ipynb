{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation by Direct Solution of Bellman Equation.\n",
    "\n",
    "Let us consider an Agent that has to navigate a Grid World with 42 states as shown below. The Agent starts from the bottom left and has to reach the top right corner without stepping on the cells with negative reward. If the Agent falls into the shaded state it will bounce back to the previous state.<br>\n",
    "\n",
    "![title](Grid_World.png)\n",
    "\n",
    " At any moment, Agent can take four actions - Up(^), Left(<), Down(V),right(>). <br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "T = np.load(\"./T.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume that a Transition Model is given to the System. Below, we examine the Transition Probabilities of some states.<br>\n",
    " \n",
    " From the Terminal State (0,6), it does not move anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 42, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition Probability of Terminal State is made to be zero for all actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You can examine that the Agent moves to the intended state with 0.8 Probability and moves to random direction with 0.1 probability. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_policy_eval_BellMan(p, r, T, gamma):\n",
    "    \"Solving the Bellman Equation directly\"\n",
    "    x = np.zeros(42)\n",
    "    for s in range(42):\n",
    "        if not np.isnan(p[s]):\n",
    "            action = int(p[s])\n",
    "            x[s] = np.linalg.solve(np.identity(42) - gamma*T[:,:,action], r)[s]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 100\n",
      "Gamma: 0.999\n",
      "Epsilon: 0.0001\n",
      "===================================================\n",
      "Estimated value function\n",
      "[[-16.57198381  -0.17304185  -0.12320832  -0.04        -0.28438129\n",
      "    0.           1.10623233]\n",
      " [ -0.57844257  -0.04         0.          -0.04        -0.31388463\n",
      "   -0.10832852  -0.59315493]\n",
      " [  0.2068669    0.30117132  -3.45777021  -2.99670873  -3.45316587\n",
      "   -0.22855119  -1.13471744]\n",
      " [  0.          -8.70030528   2.18971829   0.          -3.12559507\n",
      "   -2.22931184  -1.32977286]\n",
      " [  0.52564885  -4.44974174  -0.0649005   -1.57358793  -6.03749128\n",
      "   -0.54859993  -0.08469095]\n",
      " [ -0.04952512   0.06296128   0.          -4.06561861   0.\n",
      "    0.04064972  -0.05441301]]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.999\n",
    "iteration_it = 0\n",
    "\n",
    "#Generate the first policy randomly\n",
    "# Nan=Nothing, -1=Terminal, 0=Up, 1=Left, 2=Down, 3=Right\n",
    "p = np.random.randint(0, 4, size=(42)).astype(np.float32)\n",
    "p[5]=p[9]=p[21]=p[24]=p[37]=p[39] = np.NaN\n",
    "p[6]=p[4]=p[17]=p[20]=p[26] = -1 #terminal states\n",
    "\n",
    "#Value function initialised to zero\n",
    "v = np.array([0.0, 0.0, 0.0,  0.0,0.0, 0.0, 0.0,\n",
    "              0.0, 0.0, 0.0,  0.0,0.0, 0.0, 0.0,\n",
    "              0.0, 0.0, 0.0,  0.0,0.0, 0.0, 0.0,\n",
    "              0.0, 0.0, 0.0,  0.0,0.0, 0.0, 0.0,\n",
    "              0.0, 0.0, 0.0,  0.0,0.0, 0.0, 0.0,\n",
    "              0.0, 0.0, 0.0,  0.0,0.0, 0.0, 0.0])\n",
    "\n",
    "#let us assign appropriate rewards\n",
    "r = np.array([-0.04, -0.04, -0.04,  -0.04,  -1.0,   0.0,  +1.0,\n",
    "              -0.04, -0.04,   0.0,  -0.04, -0.04, -0.04, -0.04,\n",
    "              -0.04, -0.04, -0.04,   -1.0, -0.04, -0.04,  -1.0,\n",
    "                0.0, -0.04, -0.04,    0.0, -0.04,  -1.0, -0.04,\n",
    "              -0.04, -0.04, -0.04,  -0.04, -0.04, -0.04, -0.04,\n",
    "              -0.04, -0.04,   0.0,  -0.04,   0.0, -0.04, -0.04])\n",
    "unchanged = False\n",
    "# while True:\n",
    "for it in range(0,100):\n",
    "    iteration_it +=1\n",
    "    epsilon = 0.0001\n",
    "    #1- Policy Evaluation\n",
    "    v1 = v.copy()\n",
    "    #Direct solution\n",
    "    v = return_policy_eval_BellMan(p, r, T, gamma)\n",
    "\n",
    "print(\"Iterations: \" + str(iteration_it))\n",
    "print(\"Gamma: \" + str(gamma))\n",
    "print(\"Epsilon: \" + str(epsilon))\n",
    "print(\"===================================================\")\n",
    "print(\"Estimated value function\")\n",
    "print(v.reshape(6,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-2.32186561  -1.01791823  -0.12320832  -0.04       -0.28438129  0.0           1.10623233\n",
    "-2.3689515   -0.04         0.0         -0.04       -0.31388463 -0.06857593   -0.04284566\n",
    "-5.78604803  -1.99191225  -3.45777021  -2.99670873 -1.5905784  -1.3114059    -1.13471744\n",
    " 0.0         -0.10334749   2.18971829   0.0         1.03341728 -2.22931184   -1.32977286\n",
    "-4.54138151  -0.09296731   0.54903187   0.45820533 -0.06467763 -0.09113937   -0.08469095\n",
    "-1.12512904   0.06296128   0.0          0.08982413  0.0        -0.14084028   -0.05760154"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
