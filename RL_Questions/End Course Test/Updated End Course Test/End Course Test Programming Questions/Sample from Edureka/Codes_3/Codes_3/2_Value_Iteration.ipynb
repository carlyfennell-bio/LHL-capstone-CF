{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration \n",
    "\n",
    "This notebook gives a preview of finding Optimal Policy through Value Iteration. The Agent is on a 4*4 grid and its goal is to reach the terminal state marked with solid black fill.\n",
    "![title](images/gridworld.png)\n",
    "\n",
    "1.The Agent can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).<br> \n",
    "2.Any action that takes an Agent beyond the grid will result in the Agent staying in the same state.<br>\n",
    "3.Agent receives a reward of -1 at each step until it reaches the terminal state.<br><br><br>\n",
    "     Let us try to find a policy that can take our Agent to the terminal state and also compute the Value Function for the same using Value Iteration method. We would cover this in detail in subsequent modules,however the demo is provided now to get an illustration of how an RL problem can be solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gridWorld import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Arguments:\n",
    "        env: OpenAI env\n",
    "            env.numStates is a number of states in the environment \n",
    "            env.numActions is a number of actions in the environment\n",
    "theta: We stop evaluation once our value function change is less than theta for all states\n",
    "        discount_factor: Gamma discount factor\n",
    "        env.model[state][action] is a list of transition tuples (prob, next_state, reward, done)\n",
    "        Returns:\n",
    "        A tuple (policy, value_fn) of the optimal policy and the optimal value function\n",
    "\"\"\"\n",
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "# Helper function to calculate the value for all actions in a given state    \n",
    "    def compute_value_fn_update(state,value_fn):\n",
    "        value_fn_update = np.zeros(env.numActions)\n",
    "        for action in range(env.numActions):\n",
    "            for prob,next_state,reward,done in env.model[state][action]:\n",
    "                value_fn_update[action] += prob * (reward + discount_factor * value_fn[next_state])\n",
    "                \n",
    "        return value_fn_update \n",
    "    \n",
    "    value_fn = np.zeros(env.numStates)\n",
    "    while True:\n",
    "# Stopping Condition        \n",
    "        delta = 0\n",
    "# Update each state        \n",
    "        for state in range(env.numStates):\n",
    "# Find the best action\n",
    "            action_values = compute_value_fn_update(state, value_fn)\n",
    "            best_action_value = np.max(action_values)\n",
    "# Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - value_fn[state]))\n",
    "# Update the value function\n",
    "            value_fn[state] = best_action_value        \n",
    "# Check if we can stop       \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Create a deterministic policy by using the optimal value function\n",
    "    policy = np.zeros([env.numStates, env.numActions])\n",
    "    for state in range(env.numStates):\n",
    "    # Find the best action for this state\n",
    "        A = compute_value_fn_update(state, value_fn)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        policy[state, best_action] = 1.0\n",
    "    \n",
    "    return policy, value_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will learn about value iteration in the subsquent module, but below you can see that value iteration is able to learn a policy that would take the agent to terminal state starting from any internal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "policy, value_fn = value_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy grid (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 2]\n",
      " [0 0 0 2]\n",
      " [0 0 1 2]\n",
      " [0 1 1 0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Policy grid (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also compute the value function for each state that corresponds to the number of steps required for the agent to reach terminal state since the reward is -1 for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Grid Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\" Grid Value Function:\")\n",
    "print(value_fn.reshape(env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
