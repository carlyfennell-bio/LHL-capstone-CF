{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference  Control using  SARSA\n",
    "\n",
    "In this demo,we estimate the optimal policy starting from a random policy using SARSA algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gridworld import GridWorld\n",
    "np.set_printoptions(precision=3,suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridWorld\n",
    "\n",
    "\n",
    "1.The robot moves one step in the world based on the action given.<br>\n",
    "2.The action can be 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT <br>\n",
    "3.The environment is stochastic , hence when we say up,it goes up with a specified probability <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value function Update (State-action-reward-state-action)\n",
    "\n",
    "Update the  state-action function by looking up the value corresponding to new observation. <br>\n",
    "When we move from state(st) by taking an action, we get an reward and move to a new state that gives us new observation. We use this to look up the state_action matric (q-value) to compute the error. <br>\n",
    "The state_action matrix corresponding to previous state-action is updated according to this error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state_action(state_action_matrix, observation, new_observation, \n",
    "                   action, new_action, reward, alpha, gamma):\n",
    "    '''Return the updated utility matrix\n",
    "    '''\n",
    "    #Getting the values of Q at t and at t+1\n",
    "    col = observation[1] + (observation[0]*4)\n",
    "    q = state_action_matrix[action ,col]\n",
    "    col_t1 = new_observation[1] + (new_observation[0]*4)\n",
    "    q_t1 = state_action_matrix[new_action ,col_t1]\n",
    "    #Applying the update rule\n",
    "    state_action_matrix[action ,col] += \\\n",
    "        alpha * (reward + gamma * q_t1 - q)\n",
    "    return state_action_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have updated state_actinon matrix, we look up the matrix, find the best action(greedy update) and update our policy to reflect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(policy_matrix, state_action_matrix, observation):\n",
    "    col = observation[1] + (observation[0]*4)\n",
    "    #find  the action with the highest utility\n",
    "    best_action = np.argmax(state_action_matrix[:, col])\n",
    "    #Updating the policy\n",
    "    policy_matrix[observation[0], observation[1]] = best_action\n",
    "    return policy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_epsilon_greedy_action(policy_matrix, observation, epsilon=0.1):\n",
    "  \n",
    "    tot_actions = int(np.nanmax(policy_matrix) + 1)\n",
    "    action = int(policy_matrix[observation[0], observation[1]])\n",
    "    non_greedy_prob = epsilon / tot_actions\n",
    "    greedy_prob = 1 - epsilon + non_greedy_prob\n",
    "    weight_array = np.full((tot_actions), non_greedy_prob)\n",
    "    weight_array[action] = greedy_prob\n",
    "    return np.random.choice(tot_actions, 1, p=weight_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Matrix:\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0. -1.  0.  1.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = GridWorld(3, 4)\n",
    "\n",
    "#Define the state matrix\n",
    "state_matrix = np.zeros((3,4))\n",
    "state_matrix[0, 3] = 1\n",
    "state_matrix[1, 3] = 1\n",
    "state_matrix[1, 1] = -1\n",
    "print(\"State Matrix:\")\n",
    "print(state_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Matrix:\n",
      "[[-0.04 -0.04 -0.04  1.  ]\n",
      " [-0.04 -0.04 -0.04 -1.  ]\n",
      " [-0.04 -0.04 -0.04 -0.04]]\n"
     ]
    }
   ],
   "source": [
    "#Define the reward matrix\n",
    "reward_matrix = np.full((3,4), -0.04)\n",
    "reward_matrix[0, 3] = 1\n",
    "reward_matrix[1, 3] = -1\n",
    "print(\"Reward Matrix:\")\n",
    "print(reward_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the transition matrix\n",
    "transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                              [0.1, 0.8, 0.1, 0.0],\n",
    "                              [0.0, 0.1, 0.8, 0.1],\n",
    "                              [0.1, 0.0, 0.1, 0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Matrix:\n",
      "[[ 2.  1.  1. -1.]\n",
      " [ 0. nan  1. -1.]\n",
      " [ 2.  1.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#Random policy to start with \n",
    "policy_matrix = np.random.randint(low=0, high=4, size=(3, 4)).astype(np.float32)\n",
    "policy_matrix[1,1] = np.NaN #NaN for the obstacle at (1,1)\n",
    "policy_matrix[0,3] = policy_matrix[1,3] = -1 #No action for the terminal states\n",
    "print(\"Policy Matrix:\")\n",
    "print(policy_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.setStateMatrix(state_matrix)\n",
    "env.setRewardMatrix(reward_matrix)\n",
    "env.setTransitionMatrix(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_matrix = np.zeros((4,12))\n",
    "visit_counter_matrix = np.zeros((4,12))\n",
    "gamma = 0.999\n",
    "alpha = 0.001 \n",
    "tot_epoch = 5000\n",
    "print_epoch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State-Action matrix after 1 iterations:\n",
      "[[ 0.     0.     0.     0.    -0.     0.     0.     0.    -0.     0.\n",
      "   0.     0.   ]\n",
      " [ 0.     0.     0.     0.    -0.     0.    -0.001  0.    -0.    -0.\n",
      "  -0.     0.   ]\n",
      " [-0.     0.     0.     0.    -0.     0.     0.     0.    -0.     0.\n",
      "   0.     0.   ]\n",
      " [ 0.     0.     0.     0.    -0.     0.     0.     0.     0.     0.\n",
      "   0.     0.   ]]\n",
      "Policy matrix after 1 iterations:\n",
      "[[ 0.  1.  1. -1.]\n",
      " [ 3. nan  0. -1.]\n",
      " [ 3.  0.  0.  0.]]\n",
      "\n",
      "State-Action matrix after 1001 iterations:\n",
      "[[-0.007 -0.001  0.     0.    -0.015  0.     0.052  0.    -0.018 -0.017\n",
      "  -0.012 -0.039]\n",
      " [-0.003  0.1    0.536  0.    -0.015  0.    -0.009  0.    -0.018 -0.017\n",
      "  -0.012 -0.039]\n",
      " [-0.007 -0.001  0.     0.    -0.015  0.    -0.009  0.    -0.018 -0.017\n",
      "  -0.012 -0.039]\n",
      " [-0.007 -0.001  0.     0.    -0.015  0.    -0.008  0.    -0.018 -0.017\n",
      "  -0.012 -0.039]]\n",
      "Policy matrix after 1001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 2.  3.  0.  3.]]\n",
      "\n",
      "State-Action matrix after 2001 iterations:\n",
      "[[-0.007 -0.001  0.     0.    -0.019  0.     0.217  0.    -0.024 -0.022\n",
      "   0.014 -0.054]\n",
      " [ 0.035  0.282  0.77   0.    -0.019  0.    -0.009  0.    -0.024 -0.022\n",
      "  -0.012 -0.054]\n",
      " [-0.007 -0.001  0.     0.    -0.019  0.    -0.009  0.    -0.024 -0.022\n",
      "  -0.012 -0.053]\n",
      " [-0.007 -0.001  0.     0.    -0.019  0.    -0.008  0.    -0.024 -0.022\n",
      "  -0.012 -0.053]]\n",
      "Policy matrix after 2001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 2.  1.  0.  3.]]\n",
      "\n",
      "State-Action matrix after 3001 iterations:\n",
      "[[-0.007 -0.001  0.     0.    -0.013  0.     0.384  0.    -0.026 -0.022\n",
      "   0.08  -0.06 ]\n",
      " [ 0.098  0.437  0.86   0.    -0.019  0.    -0.009  0.    -0.026 -0.018\n",
      "  -0.012 -0.061]\n",
      " [-0.007 -0.001  0.     0.    -0.019  0.    -0.009  0.    -0.026 -0.022\n",
      "  -0.012 -0.06 ]\n",
      " [-0.007 -0.001  0.     0.    -0.019  0.    -0.008  0.    -0.026 -0.022\n",
      "  -0.012 -0.057]]\n",
      "Policy matrix after 3001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 1.  1.  0.  3.]]\n",
      "\n",
      "State-Action matrix after 4001 iterations:\n",
      "[[-0.007 -0.001  0.     0.     0.005  0.     0.486  0.    -0.027 -0.022\n",
      "   0.162 -0.06 ]\n",
      " [ 0.183  0.569  0.908  0.    -0.019  0.    -0.009  0.    -0.027 -0.001\n",
      "  -0.012 -0.061]\n",
      " [-0.007 -0.001  0.     0.    -0.019  0.    -0.009  0.    -0.027 -0.022\n",
      "  -0.012 -0.06 ]\n",
      " [-0.007 -0.001  0.     0.    -0.019  0.    -0.008  0.    -0.027 -0.022\n",
      "  -0.012 -0.052]]\n",
      "Policy matrix after 4001 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  1.  0.  3.]]\n",
      "State-Action matrix after 5000 iterations:\n",
      "[[-0.007 -0.001  0.     0.     0.047  0.     0.56   0.    -0.026 -0.022\n",
      "   0.239 -0.06 ]\n",
      " [ 0.282  0.673  0.925  0.    -0.019  0.    -0.009  0.    -0.027  0.021\n",
      "  -0.012 -0.061]\n",
      " [-0.007 -0.001  0.     0.    -0.019  0.    -0.009  0.    -0.027 -0.022\n",
      "  -0.012 -0.06 ]\n",
      " [-0.007 -0.001  0.     0.    -0.019  0.    -0.008  0.    -0.027 -0.022\n",
      "  -0.012 -0.04 ]]\n",
      "Policy matrix after 5000 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  1.  0.  3.]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(tot_epoch):\n",
    "#Reset and return the first observation\n",
    "    observation = env.reset(exploring_starts=True)\n",
    "    for step in range(1000):\n",
    "        #pdb.set_trace()\n",
    "        #Take the action from the action matrix\n",
    "        action = int(policy_matrix[observation[0], observation[1]])\n",
    "        #Move one step in the environment and get obs,reward and new action\n",
    "        new_observation, reward, done = env.step(action)\n",
    "        new_action = int(policy_matrix[new_observation[0], new_observation[1]])\n",
    "        #Updating the state-action matrix\n",
    "        state_action_matrix = update_state_action(state_action_matrix, \n",
    "                                          observation, new_observation, \n",
    "                                          action, new_action, \n",
    "                                          reward, alpha, gamma)\n",
    "        #Updating the policy\n",
    "        policy_matrix = update_policy(policy_matrix, \n",
    "                              state_action_matrix, \n",
    "                              observation)\n",
    "        observation = new_observation\n",
    "        if done: break\n",
    "\n",
    "    if(epoch % print_epoch == 0):\n",
    "        print(\"\")\n",
    "        print(\"State-Action matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "        print(state_action_matrix)\n",
    "        print(\"Policy matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "        print(policy_matrix)\n",
    "\n",
    "\n",
    "#Time to check the utility matrix obtained\n",
    "print(\"State-Action matrix after \" + str(tot_epoch) + \" iterations:\")\n",
    "print(state_action_matrix)\n",
    "print(\"Policy matrix after \" + str(tot_epoch) + \" iterations:\")\n",
    "print(policy_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
