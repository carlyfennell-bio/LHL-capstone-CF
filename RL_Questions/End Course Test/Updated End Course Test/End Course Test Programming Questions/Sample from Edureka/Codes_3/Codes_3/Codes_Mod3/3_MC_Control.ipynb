{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MonteCarlo Control\n",
    "\n",
    "Control is the process of finding the best policy for a given environment. We follow generalised policy iteration and alternate between policy evaluation and policy improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gridworld import GridWorld\n",
    "np.set_printoptions(precision=3,suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return computation function now takes as input a list containing a 3-tuple (state,action,reward) and the discount factor gamma, the output is a value representing the return from that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_return(state_list, gamma):\n",
    "    counter = 0\n",
    "    return_value = 0\n",
    "    for visit in state_list:\n",
    "        reward = visit[2]\n",
    "        return_value += reward * np.power(gamma, counter)\n",
    "        counter += 1\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a policy and improve it by taking a greedy action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(episode_list, policy_matrix, state_action_matrix):\n",
    "    \n",
    "    for visit in episode_list:\n",
    "        observation = visit[0]\n",
    "        col = observation[1] + (observation[0]*4)\n",
    "        if(policy_matrix[observation[0], observation[1]] != -1):      \n",
    "            policy_matrix[observation[0], observation[1]] = \\\n",
    "                np.argmax(state_action_matrix[:,col])\n",
    "    return policy_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1.Lets create a grid world, the states marked 1 are terminal state and those marked -1 contain obstacles. <br>\n",
    "2.The agent receives a reward of -0.04 for every move from non-terminal states <br>\n",
    "3.The  actions are UP(0), RIGHT(1), DOWN(2) and LEFT(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Matrix:\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0. -1.  0.  1.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "Reward Matrix:\n",
      "[[-0.04 -0.04 -0.04  1.  ]\n",
      " [-0.04 -0.04 -0.04 -1.  ]\n",
      " [-0.04 -0.04 -0.04 -0.04]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = GridWorld(3, 4)\n",
    "\n",
    "#Define the state matrix\n",
    "state_matrix = np.zeros((3,4))\n",
    "state_matrix[0, 3] = 1\n",
    "state_matrix[1, 3] = 1\n",
    "state_matrix[1, 1] = -1\n",
    "print(\"State Matrix:\")\n",
    "print(state_matrix)\n",
    "\n",
    "#Define the reward matrix\n",
    "reward_matrix = np.full((3,4), -0.04)\n",
    "reward_matrix[0, 3] = 1\n",
    "reward_matrix[1, 3] = -1\n",
    "print(\"Reward Matrix:\")\n",
    "print(reward_matrix)\n",
    "\n",
    "#Define the transition matrix\n",
    "transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                              [0.1, 0.8, 0.1, 0.0],\n",
    "                              [0.0, 0.1, 0.8, 0.1],\n",
    "                              [0.1, 0.0, 0.1, 0.8]])\n",
    "\n",
    "#Random policy\n",
    "policy_matrix = np.random.randint(low=0, high=4, size=(3, 4)).astype(np.float32)\n",
    "policy_matrix[1,1] = np.NaN #NaN for the obstacle at (1,1)\n",
    "policy_matrix[0,3] = policy_matrix[1,3] = -1 #No action for the terminal states\n",
    "\n",
    "#Set the matrices in the world\n",
    "env.setStateMatrix(state_matrix)\n",
    "env.setRewardMatrix(reward_matrix)\n",
    "env.setTransitionMatrix(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy matrix after 1 iterations:\n",
      "[[ 1.  0.  3. -1.]\n",
      " [ 1. nan  1. -1.]\n",
      " [ 2.  3.  0.  3.]]\n",
      "\n",
      "Policy matrix after 21 iterations:\n",
      "[[ 2.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 0.  2.  0.  0.]]\n",
      "\n",
      "Policy matrix after 41 iterations:\n",
      "[[ 2.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  2.  0.  0.]]\n",
      "\n",
      "Policy matrix after 61 iterations:\n",
      "[[ 2.  2.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  2.  0.  0.]]\n",
      "\n",
      "Policy matrix after 81 iterations:\n",
      "[[ 2.  2.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 101 iterations:\n",
      "[[ 2.  2.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 121 iterations:\n",
      "[[ 2.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 141 iterations:\n",
      "[[ 2.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 161 iterations:\n",
      "[[ 2.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 181 iterations:\n",
      "[[ 2.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 201 iterations:\n",
      "[[ 2.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 221 iterations:\n",
      "[[ 2.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 241 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 261 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 281 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 301 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 321 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 341 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 361 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 381 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 401 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 421 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 441 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 461 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 481 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 501 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 521 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 541 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 561 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 581 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 601 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 621 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 641 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 661 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  0.]]\n",
      "\n",
      "Policy matrix after 681 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  3.]]\n",
      "\n",
      "Policy matrix after 701 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  3.]]\n",
      "\n",
      "Policy matrix after 721 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  3.]]\n",
      "\n",
      "Policy matrix after 741 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  3.]]\n",
      "\n",
      "Policy matrix after 761 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  3.]]\n",
      "\n",
      "Policy matrix after 781 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  3.]]\n",
      "\n",
      "Policy matrix after 801 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  3.]]\n",
      "\n",
      "Policy matrix after 821 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  3.]]\n",
      "\n",
      "Policy matrix after 841 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  3.]]\n",
      "\n",
      "Policy matrix after 861 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  3.]]\n",
      "\n",
      "Policy matrix after 881 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 2. nan  0. -1.]\n",
      " [ 1.  1.  0.  3.]]\n",
      "\n",
      "Policy matrix after 901 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 3. nan  0. -1.]\n",
      " [ 1.  1.  0.  3.]]\n",
      "\n",
      "Policy matrix after 921 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 3. nan  0. -1.]\n",
      " [ 1.  1.  0.  3.]]\n",
      "\n",
      "Policy matrix after 941 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 3. nan  0. -1.]\n",
      " [ 1.  1.  0.  3.]]\n",
      "\n",
      "Policy matrix after 961 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 3. nan  0. -1.]\n",
      " [ 1.  1.  0.  3.]]\n",
      "\n",
      "Policy matrix after 981 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 3. nan  0. -1.]\n",
      " [ 1.  1.  0.  3.]]\n",
      "value matrix after 1000 iterations:\n",
      "[[-1.535e+00  6.154e-01  8.329e-01  3.733e+09 -4.223e-01  8.757e+09\n",
      "   6.951e-01  9.329e+09 -1.149e-02 -3.544e-01  5.200e-01 -8.394e-01]\n",
      " [ 7.087e-01  9.241e-01  9.544e-01  8.967e+09 -4.430e-01  8.484e+09\n",
      "  -7.243e-01  8.953e+09  3.376e-01  4.587e-01 -4.129e-01 -1.262e+00]\n",
      " [ 2.331e-01  8.953e-01  4.866e-01  9.524e+09  2.844e-01  3.947e+09\n",
      "  -3.197e-01  7.533e+09 -5.692e-01  1.803e-01 -5.049e-01 -7.081e-01]\n",
      " [-6.271e-01 -1.977e-01  5.674e-01  3.098e+09  3.974e-01  8.925e+09\n",
      "   3.547e-01  8.542e+09 -6.455e-01 -3.380e-01 -2.929e-01  1.837e-01]]\n",
      "Policy matrix after 1000 iterations:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 3. nan  0. -1.]\n",
      " [ 1.  1.  0.  3.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "state_action_matrix = np.random.random_sample((4,12)) # Q\n",
    "#init with 1.0e-10 to avoid division by zero\n",
    "running_mean_matrix = np.full((4,12), 1.0e-10) \n",
    "gamma = 0.999\n",
    "tot_epoch = 1000\n",
    "print_epoch = 20\n",
    "\n",
    "for epoch in range(tot_epoch):\n",
    "    #Starting a new episode\n",
    "    episode_list = list()\n",
    "    #Reset and return the first observation and reward\n",
    "    observation = env.reset(exploring_starts=True)\n",
    "    #action = np.random.choice(4, 1)\n",
    "    #action = policy_matrix[observation[0], observation[1]]\n",
    "    #episode_list.append((observation, action, reward))\n",
    "    is_starting = True\n",
    "    for _ in range(1000):\n",
    "        #Take the action from the action matrix\n",
    "        action = policy_matrix[observation[0], observation[1]]\n",
    "        #If the episode just started then it is\n",
    "            #necessary to choose a random action (exploring starts)\n",
    "        if(is_starting): \n",
    "            action = np.random.randint(0, 4)\n",
    "            is_starting = False      \n",
    "        #Move one step in the environment and get obs and reward\n",
    "        new_observation, reward, done = env.step(action)\n",
    "        #Append the visit in the episode list\n",
    "        episode_list.append((observation, action, reward))\n",
    "        observation = new_observation\n",
    "        if done: break\n",
    "    #The episode is finished, now estimating the utilities\n",
    "    counter = 0\n",
    "    #Checkup to identify if it is the first visit to a state\n",
    "    checkup_matrix = np.zeros((4,12))\n",
    "    #This cycle is the implementation of First-Visit MC.\n",
    "    #For each state stored in the episode list check if it\n",
    "    #is the rist visit and then estimate the return.\n",
    "    for visit in episode_list:\n",
    "        observation = visit[0]\n",
    "        action = visit[1]\n",
    "        col = int(observation[1] + (observation[0]*4))\n",
    "        row = int(action)\n",
    "        if(checkup_matrix[row, col] == 0):\n",
    "            return_value = get_return(episode_list[counter:], gamma)\n",
    "            running_mean_matrix[row, col] += 1\n",
    "            state_action_matrix[row, col] += return_value\n",
    "            checkup_matrix[row, col] = 1\n",
    "        counter += 1\n",
    "    #Policy Update\n",
    "    policy_matrix = improve_policy(episode_list, \n",
    "                                  policy_matrix, \n",
    "                                  state_action_matrix/running_mean_matrix)\n",
    "    #Printing\n",
    "    if(epoch % print_epoch == 0):\n",
    "        print(\"\")\n",
    "        #print(\"State-Action matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "        #print(state_action_matrix / running_mean_matrix)\n",
    "        print(\"Policy matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "        print(policy_matrix)\n",
    "        \n",
    "#Time to check the value  matrix obtained\n",
    "print(\"value matrix after \" + str(tot_epoch) + \" iterations:\")\n",
    "print(state_action_matrix / running_mean_matrix)\n",
    "\n",
    "print(\"Policy matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "print(policy_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
