{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MonteCarlo Prediction\n",
    "\n",
    "Given the environment description (policy and transition probabilities ) prediction is the computations done to evaluate the value function when we follow a given policy. This demo intends to evaluate policy using first visit montaecarlo estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gridworld import GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return computation function takes as input a list containing a tuple (state, reward) and the discount factor gamma, the output is a value representing the return from that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_return(state_list, gamma):\n",
    "    counter = 0\n",
    "    return_value = 0\n",
    "    for visit in state_list:\n",
    "        reward = visit[1]\n",
    "        return_value += reward * np.power(gamma, counter)\n",
    "        counter += 1\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Lets create a grid world, the states marked 1 are terminal state and those marked -1 contain obstacles. <br>\n",
    "2.The agent receives a reward of -0.04 for every move from non-terminal states <br>\n",
    "3.The  actions are UP(0), RIGHT(1), DOWN(2) and LEFT(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Matrix:\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0. -1.  0.  1.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(3, 4)\n",
    "\n",
    "#Define the state matrix\n",
    "state_matrix = np.zeros((3,4))\n",
    "state_matrix[0, 3] = 1\n",
    "state_matrix[1, 3] = 1\n",
    "state_matrix[1, 1] = -1\n",
    "print(\"State Matrix:\")\n",
    "print(state_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also define rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Matrix:\n",
      "[[-0.04 -0.04 -0.04  1.  ]\n",
      " [-0.04 -0.04 -0.04 -1.  ]\n",
      " [-0.04 -0.04 -0.04 -0.04]]\n"
     ]
    }
   ],
   "source": [
    " #Define the reward matrix\n",
    "reward_matrix = np.full((3,4), -0.04)\n",
    "reward_matrix[0, 3] = 1\n",
    "reward_matrix[1, 3] = -1\n",
    "print(\"Reward Matrix:\")\n",
    "print(reward_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define the transition probabilities for four possible actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Define the transition matrix\n",
    "transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                              [0.1, 0.8, 0.1, 0.0],\n",
    "                              [0.0, 0.1, 0.8, 0.1],\n",
    "                              [0.1, 0.0, 0.1, 0.8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this grid is designed by us, we know the best policy to follow. Lets define it now and evaluate it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1., -1.],\n",
       "       [ 0., nan,  0., -1.],\n",
       "       [ 0.,  3.,  3.,  3.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_matrix = np.array([[1,      1,  1,  -1],\n",
    "                              [0, np.NaN,  0,  -1],\n",
    "                              [0,      3,  3,   3]])\n",
    "policy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = GridWorld(3, 4)\n",
    "env.reset()\n",
    "env.setStateMatrix(state_matrix)\n",
    "env.setRewardMatrix(reward_matrix)\n",
    "env.setTransitionMatrix(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -  -  -  * \n",
      " -  #  -  * \n",
      " â—‹  -  -  - \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "value matrix after 1 iterations:\n",
      "[[0.87712296 0.918041   0.959      1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "value matrix after 1001 iterations:\n",
      "[[ 0.81726075  0.87116267  0.92616747  1.        ]\n",
      " [ 0.76837552  0.          0.70963449 -1.        ]\n",
      " [ 0.70772261  0.64909468  0.56512142  0.34041059]]\n",
      "\n",
      "value matrix after 2001 iterations:\n",
      "[[ 0.81667227  0.86922313  0.92128329  1.        ]\n",
      " [ 0.77181359  0.          0.69149888 -1.        ]\n",
      " [ 0.71584488  0.6645446   0.60574944  0.4352794 ]]\n",
      "\n",
      "value matrix after 3001 iterations:\n",
      "[[ 0.81029834  0.8664007   0.9194719   1.        ]\n",
      " [ 0.76478083  0.          0.66775242 -1.        ]\n",
      " [ 0.70840809  0.65929414  0.61149046  0.45721452]]\n",
      "\n",
      "value matrix after 4001 iterations:\n",
      "[[ 0.811802    0.86631311  0.91777017  1.        ]\n",
      " [ 0.76576608  0.          0.65287122 -1.        ]\n",
      " [ 0.70965258  0.6592462   0.60709053  0.4649459 ]]\n",
      "\n",
      "value matrix after 5001 iterations:\n",
      "[[ 0.80798843  0.86262257  0.91636549  1.        ]\n",
      " [ 0.76096217  0.          0.66773465 -1.        ]\n",
      " [ 0.70275503  0.65275262  0.61019053  0.4279703 ]]\n",
      "\n",
      "value matrix after 6001 iterations:\n",
      "[[ 0.80879087  0.86468517  0.91824417  1.        ]\n",
      " [ 0.76070826  0.          0.67754284 -1.        ]\n",
      " [ 0.70066488  0.64850726  0.6004125   0.32322451]]\n",
      "\n",
      "value matrix after 7001 iterations:\n",
      "[[ 0.80705012  0.86376895  0.9172426   1.        ]\n",
      " [ 0.75794718  0.          0.6668633  -1.        ]\n",
      " [ 0.69850926  0.64268126  0.6007173   0.31081607]]\n",
      "\n",
      "value matrix after 8001 iterations:\n",
      "[[ 0.80708854  0.86336951  0.91665958  1.        ]\n",
      " [ 0.75806271  0.          0.65953303 -1.        ]\n",
      " [ 0.69805985  0.64232073  0.60010042  0.29663728]]\n",
      "\n",
      "value matrix after 9001 iterations:\n",
      "[[ 0.80623407  0.86342697  0.91628322  1.        ]\n",
      " [ 0.75680417  0.          0.65999858 -1.        ]\n",
      " [ 0.69814288  0.64257222  0.60024779  0.28004838]]\n",
      "value matrix after 10000 iterations:\n",
      "[[ 0.80615252  0.8638346   0.91653167  1.        ]\n",
      " [ 0.75575492  0.          0.65866191 -1.        ]\n",
      " [ 0.6966432   0.64247364  0.59718136  0.30825656]]\n"
     ]
    }
   ],
   "source": [
    "value_matrix = np.zeros((3,4))\n",
    "#init with 1.0e-10 to avoid division by zero\n",
    "running_mean_matrix = np.full((3,4), 1.0e-10) \n",
    "gamma = 0.999\n",
    "tot_epoch = 10000\n",
    "print_epoch = 1000\n",
    "\n",
    "\n",
    "for epoch in range(tot_epoch):\n",
    "    #Starting a new episode\n",
    "    episode_list = list()\n",
    "    #Reset and return the first observation and reward\n",
    "    observation = env.reset(exploring_starts=True)\n",
    "    for _ in range(1000):\n",
    "        #Take the action from the action matrix\n",
    "        action = policy_matrix[observation[0], observation[1]]\n",
    "        #Move one step in the environment and get obs and reward\n",
    "        observation, reward, done = env.step(action)\n",
    "        #Append the visit in the episode list\n",
    "        episode_list.append((observation, reward))\n",
    "        if done: break\n",
    "    #The episode is finished, now estimating the value function\n",
    "    counter = 0\n",
    "    #Checkup to identify if it is the first visit to a state\n",
    "    checkup_matrix = np.zeros((3,4))\n",
    "    #This cycle is the implementation of First-Visit MC.\n",
    "    #For each state stored in the episode list check if it\n",
    "    #is the first visit and then estimate the return.\n",
    "    for visit in episode_list:\n",
    "        observation = visit[0]\n",
    "        row = observation[0]\n",
    "        col = observation[1]\n",
    "        reward = visit[1]\n",
    "        if(checkup_matrix[row, col] == 0):\n",
    "            return_value = get_return(episode_list[counter:], gamma)\n",
    "            running_mean_matrix[row, col] += 1\n",
    "            value_matrix[row, col] += return_value\n",
    "            checkup_matrix[row, col] = 1\n",
    "        counter += 1\n",
    "    if(epoch % print_epoch == 0):\n",
    "        print(\"\")\n",
    "        print(\"value matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "        print(value_matrix / running_mean_matrix)\n",
    "#Time to check the value matrix obtained\n",
    "print(\"value matrix after \" + str(tot_epoch) + \" iterations:\")\n",
    "print(value_matrix / running_mean_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
