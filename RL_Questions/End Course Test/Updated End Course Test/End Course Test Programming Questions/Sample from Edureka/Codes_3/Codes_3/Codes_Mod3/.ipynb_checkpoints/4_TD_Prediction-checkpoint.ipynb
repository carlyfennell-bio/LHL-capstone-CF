{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD Prediction\n",
    "\n",
    "In this demo we estimate the value function for a given policy using Temporal difference method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gridworld import GridWorld\n",
    "\n",
    "np.set_printoptions(precision=3,suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are at state s_t we take an action based on the current observation,we get an reward and move to the next state S_t+1 and also get a new_observation. Using this we look up the value function and compute the reward for state s_t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_value_matrix(value_matrix, observation, new_observation, \n",
    "                   reward, alpha, gamma):\n",
    "    \n",
    "    u = value_matrix[observation[0], observation[1]]\n",
    "    u_t1 = value_matrix[new_observation[0], new_observation[1]]\n",
    "    value_matrix[observation[0], observation[1]] += \\\n",
    "        alpha * (reward + gamma * u_t1 - u)\n",
    "    return value_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a gridworld of size (3,4). The terminal states are noted as 1 and others as zero. The agent has four possible actions (UP,DOWN,LEFT,RIGHT) and gets an reward of -0.04 at all states. One of the terminal states has a reward of +1 and other -1 (not favored). Value functions represent how long does it take for an agent to reach the terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Matrix:\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0. -1.  0.  1.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(3, 4)\n",
    "\n",
    "#Define the state matrix\n",
    "state_matrix = np.zeros((3,4))\n",
    "state_matrix[0, 3] = 1\n",
    "state_matrix[1, 3] = 1\n",
    "state_matrix[1, 1] = -1\n",
    "print(\"State Matrix:\")\n",
    "print(state_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Matrix:\n",
      "[[-0.04 -0.04 -0.04  1.  ]\n",
      " [-0.04 -0.04 -0.04 -1.  ]\n",
      " [-0.04 -0.04 -0.04 -0.04]]\n"
     ]
    }
   ],
   "source": [
    "#Define the reward matrix\n",
    "reward_matrix = np.full((3,4), -0.04)\n",
    "reward_matrix[0, 3] = 1\n",
    "reward_matrix[1, 3] = -1\n",
    "print(\"Reward Matrix:\")\n",
    "print(reward_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Matrix:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      "\n",
      "Value matrix after 1 iterations:\n",
      "[[-0.004 -0.004  0.096  0.   ]\n",
      " [-0.004  0.     0.     0.   ]\n",
      " [-0.004 -0.004 -0.004  0.   ]]\n",
      "\n",
      "Value matrix after 1001 iterations:\n",
      "[[0.872 0.927 0.916 0.   ]\n",
      " [0.825 0.    0.451 0.   ]\n",
      " [0.771 0.717 0.663 0.491]]\n",
      "\n",
      "Value matrix after 2001 iterations:\n",
      "[[0.853 0.914 0.978 0.   ]\n",
      " [0.79  0.    0.859 0.   ]\n",
      " [0.733 0.687 0.69  0.43 ]]\n",
      "\n",
      "Value matrix after 3001 iterations:\n",
      "[[0.881 0.932 0.982 0.   ]\n",
      " [0.825 0.    0.5   0.   ]\n",
      " [0.762 0.705 0.651 0.579]]\n",
      "\n",
      "Value matrix after 4001 iterations:\n",
      "[[0.869 0.926 0.964 0.   ]\n",
      " [0.819 0.    0.702 0.   ]\n",
      " [0.757 0.691 0.642 0.364]]\n",
      "\n",
      "Value matrix after 5001 iterations:\n",
      "[[0.89  0.938 0.994 0.   ]\n",
      " [0.834 0.    0.93  0.   ]\n",
      " [0.783 0.721 0.66  0.207]]\n",
      "\n",
      "Value matrix after 6001 iterations:\n",
      "[[0.861 0.926 0.992 0.   ]\n",
      " [0.794 0.    0.391 0.   ]\n",
      " [0.734 0.696 0.623 0.327]]\n",
      "\n",
      "Value matrix after 7001 iterations:\n",
      "[[0.852 0.885 0.951 0.   ]\n",
      " [0.809 0.    0.784 0.   ]\n",
      " [0.744 0.691 0.648 0.358]]\n",
      "\n",
      "Value matrix after 8001 iterations:\n",
      "[[0.835 0.846 0.905 0.   ]\n",
      " [0.785 0.    0.622 0.   ]\n",
      " [0.752 0.701 0.651 0.503]]\n",
      "\n",
      "Value matrix after 9001 iterations:\n",
      "[[0.845 0.883 0.933 0.   ]\n",
      " [0.794 0.    0.645 0.   ]\n",
      " [0.753 0.704 0.659 0.529]]\n",
      "\n",
      "Value matrix after 10001 iterations:\n",
      "[[0.863 0.929 0.995 0.   ]\n",
      " [0.801 0.    0.884 0.   ]\n",
      " [0.748 0.719 0.697 0.308]]\n",
      "\n",
      "Value matrix after 11001 iterations:\n",
      "[[0.887 0.935 0.935 0.   ]\n",
      " [0.823 0.    0.444 0.   ]\n",
      " [0.775 0.716 0.676 0.615]]\n",
      "\n",
      "Value matrix after 12001 iterations:\n",
      "[[0.867 0.918 0.99  0.   ]\n",
      " [0.811 0.    0.717 0.   ]\n",
      " [0.742 0.683 0.639 0.379]]\n",
      "\n",
      "Value matrix after 13001 iterations:\n",
      "[[0.867 0.915 0.975 0.   ]\n",
      " [0.803 0.    0.829 0.   ]\n",
      " [0.76  0.7   0.653 0.573]]\n",
      "\n",
      "Value matrix after 14001 iterations:\n",
      "[[0.862 0.907 0.978 0.   ]\n",
      " [0.82  0.    0.719 0.   ]\n",
      " [0.766 0.699 0.664 0.407]]\n",
      "\n",
      "Value matrix after 15001 iterations:\n",
      "[[0.884 0.923 0.891 0.   ]\n",
      " [0.828 0.    0.701 0.   ]\n",
      " [0.759 0.709 0.665 0.52 ]]\n",
      "\n",
      "Value matrix after 16001 iterations:\n",
      "[[0.839 0.887 0.945 0.   ]\n",
      " [0.788 0.    0.471 0.   ]\n",
      " [0.721 0.69  0.655 0.256]]\n",
      "\n",
      "Value matrix after 17001 iterations:\n",
      "[[0.869 0.94  0.997 0.   ]\n",
      " [0.827 0.    0.892 0.   ]\n",
      " [0.759 0.711 0.69  0.567]]\n",
      "\n",
      "Value matrix after 18001 iterations:\n",
      "[[0.828 0.859 0.917 0.   ]\n",
      " [0.79  0.    0.556 0.   ]\n",
      " [0.727 0.671 0.624 0.409]]\n",
      "\n",
      "Value matrix after 19001 iterations:\n",
      "[[0.83  0.89  0.968 0.   ]\n",
      " [0.778 0.    0.8   0.   ]\n",
      " [0.728 0.684 0.63  0.431]]\n",
      "\n",
      "Value matrix after 20001 iterations:\n",
      "[[0.845 0.893 0.943 0.   ]\n",
      " [0.801 0.    0.752 0.   ]\n",
      " [0.749 0.69  0.635 0.53 ]]\n",
      "\n",
      "Value matrix after 21001 iterations:\n",
      "[[0.819 0.905 0.979 0.   ]\n",
      " [0.754 0.    0.591 0.   ]\n",
      " [0.732 0.703 0.648 0.393]]\n",
      "\n",
      "Value matrix after 22001 iterations:\n",
      "[[0.837 0.869 0.905 0.   ]\n",
      " [0.798 0.    0.666 0.   ]\n",
      " [0.75  0.699 0.631 0.45 ]]\n",
      "\n",
      "Value matrix after 23001 iterations:\n",
      "[[0.859 0.929 0.985 0.   ]\n",
      " [0.815 0.    0.861 0.   ]\n",
      " [0.738 0.686 0.678 0.577]]\n",
      "\n",
      "Value matrix after 24001 iterations:\n",
      "[[0.865 0.918 0.975 0.   ]\n",
      " [0.813 0.    0.709 0.   ]\n",
      " [0.758 0.707 0.642 0.578]]\n",
      "\n",
      "Value matrix after 25001 iterations:\n",
      "[[0.845 0.923 0.98  0.   ]\n",
      " [0.792 0.    0.712 0.   ]\n",
      " [0.706 0.641 0.619 0.448]]\n",
      "\n",
      "Value matrix after 26001 iterations:\n",
      "[[0.858 0.888 0.932 0.   ]\n",
      " [0.806 0.    0.719 0.   ]\n",
      " [0.745 0.687 0.642 0.4  ]]\n",
      "\n",
      "Value matrix after 27001 iterations:\n",
      "[[0.876 0.909 0.903 0.   ]\n",
      " [0.834 0.    0.525 0.   ]\n",
      " [0.777 0.716 0.649 0.398]]\n",
      "\n",
      "Value matrix after 28001 iterations:\n",
      "[[0.869 0.921 0.984 0.   ]\n",
      " [0.82  0.    0.873 0.   ]\n",
      " [0.769 0.721 0.682 0.528]]\n",
      "\n",
      "Value matrix after 29001 iterations:\n",
      "[[0.869 0.932 0.97  0.   ]\n",
      " [0.81  0.    0.617 0.   ]\n",
      " [0.741 0.696 0.636 0.296]]\n",
      "Value matrix after 30000 iterations:\n",
      "[[0.849 0.91  0.978 0.   ]\n",
      " [0.794 0.    0.598 0.   ]\n",
      " [0.748 0.709 0.649 0.534]]\n"
     ]
    }
   ],
   "source": [
    "#Define the transition matrix\n",
    "transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                              [0.1, 0.8, 0.1, 0.0],\n",
    "                              [0.0, 0.1, 0.8, 0.1],\n",
    "                              [0.1, 0.0, 0.1, 0.8]])\n",
    "\n",
    "#Define the policy matrix\n",
    "#This is the optimal policy for world with reward=-0.04\n",
    "policy_matrix = np.array([[1,      1,  1,  -1],\n",
    "                          [0, np.NaN,  0,  -1],\n",
    "                          [0,      3,  3,   3]])\n",
    "print(\"Policy Matrix:\")\n",
    "print(policy_matrix)\n",
    "\n",
    "env.setStateMatrix(state_matrix)\n",
    "env.setRewardMatrix(reward_matrix)\n",
    "env.setTransitionMatrix(transition_matrix)\n",
    "\n",
    "value_matrix = np.zeros((3,4))\n",
    "gamma = 0.999\n",
    "alpha = 0.1 #constant step size\n",
    "tot_epoch = 30000\n",
    "print_epoch = 1000\n",
    "\n",
    "for epoch in range(tot_epoch):\n",
    "    #Reset and return the first observation\n",
    "    observation = env.reset(exploring_starts=True)\n",
    "    for step in range(1000):\n",
    "        #Take the action from the action matrix\n",
    "        action = policy_matrix[observation[0], observation[1]]\n",
    "        #Move one step in the environment and get obs and reward\n",
    "        new_observation, reward, done = env.step(action)\n",
    "        value_matrix = update_value_matrix(value_matrix, observation, \n",
    "                                        new_observation, reward, alpha, gamma)\n",
    "        observation = new_observation\n",
    "        \n",
    "        if done: break\n",
    "\n",
    "    if(epoch % print_epoch == 0):\n",
    "        print(\"\")\n",
    "        print(\"Value matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "        print(value_matrix)\n",
    "#Time to check the value matrix obtained\n",
    "print(\"Value matrix after \" + str(tot_epoch) + \" iterations:\")\n",
    "print(value_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
