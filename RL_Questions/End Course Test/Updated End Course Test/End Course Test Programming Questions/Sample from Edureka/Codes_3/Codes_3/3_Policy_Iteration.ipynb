{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "This notebook gives a preview of finding Optimal Policy through Policy Iteration. The Agent is on a 4*4 grid and its goal is to reach the terminal state marked with solid black fill.\n",
    "![title](images/gridworld.png)\n",
    "\n",
    "1.The Agent can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).<br> \n",
    "2.Any action that takes an Agent beyond the grid will result in the Agent staying in the same state.<br>\n",
    "3.Agent recieves a reward of -1 at each step until it reaches the terminal state.<br><br><br>\n",
    "     Let us try to find a policy that can take our Agent to the terminal state and also compute the Value Function for the same using Policy Iteration method. We would cover this in detail in subsequent module,however the demo is provided now to get an illustration of how an RL problem can be solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gridWorld import GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a policy, find the worthiness of states.Initialize the worthiness of states by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "        Arguments:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. \n",
    "        env.numStates : number of states in the environment\n",
    "        env.numActions: number of actions in the environment\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "\"\"\"\n",
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    # Start with a random (all 0) value function\n",
    "    value_fn = np.zeros(env.numStates)\n",
    "    while True:\n",
    "        delta =0\n",
    "    # For each state, perform a \"full backup\"\n",
    "        for state in range(env.numStates):\n",
    "            state_value =0\n",
    "    # Look at the possible next actions\n",
    "            for action,action_prob in enumerate(policy[state]):\n",
    "    # For each action, look at the possible next states...\n",
    "    # env.model[state][action] is a list of transition tuples (prob, next_state, reward, done)\n",
    "                for  prob, next_state, reward, done in env.model[state][action]:\n",
    "                    # Calculate the expected value\n",
    "                    state_value += action_prob * prob * (reward + discount_factor * value_fn[next_state])\n",
    "           # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(state_value - value_fn[state]))\n",
    "            value_fn[state] = state_value\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return value_fn\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Iteratively evaluates and improves a policy untill an Optimal Policy is found\n",
    "Arguments:\n",
    "    env: The OpenAI environment\n",
    "    policy_eval_fn: Policy Evaluation function that takes three arguments: policy,env,discount_factor\n",
    "    discount_factor: gamma discount factor\n",
    "    \n",
    "Returns:\n",
    "    A tuple (policy,value_fn)\n",
    "    policy is the optimal policy, a matrix of shape [S,A] where each state s contains a valid probability distribution \n",
    "    over actions\n",
    "    value_fn is the value function for the optimal policy\n",
    "\"\"\"\n",
    "\n",
    "def policy_iteration(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    \n",
    "    def compute_value_fn_update(state,value_fn):\n",
    "        value_fn_update = np.zeros(env.numActions)\n",
    "        for action in range(env.numActions):\n",
    "            for prob,next_state,reward,done in env.model[state][action]:\n",
    "                value_fn_update[action] += prob * (reward + discount_factor * value_fn[next_state])\n",
    "                \n",
    "        return value_fn_update \n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.numStates,env.numActions]) /env.numActions\n",
    "     \n",
    " \n",
    "    \n",
    "    while True:\n",
    "        # Evaluate the current policy, calculate the value function, call to policy_eval function\n",
    "        value_fn = policy_eval_fn(policy, env, discount_factor)\n",
    "        \n",
    "        \n",
    "        policy_stable = True\n",
    "        \n",
    "       # Policy Improvement\n",
    "    \n",
    "        for state in range(env.numStates):\n",
    "            # The best action we would take under the currect policy\n",
    "            chosen_a = np.argmax(policy[state])\n",
    "            \n",
    "            # Find the best action \n",
    "            # Ties are resolved arbitrarily\n",
    "            action_values = compute_value_fn_update(state, value_fn)\n",
    "            best_a = np.argmax(action_values)\n",
    "            \n",
    "            # Greedily update the policy\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "            policy[state] = np.eye(env.numActions)[best_a]\n",
    "        \n",
    "        # If the policy is stable we've found an optimal policy. Return it\n",
    "        if policy_stable:\n",
    "            return policy, value_fn\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in range(env.numStates):\n",
    "            state_value = 0\n",
    "            for action,action_prob in enumerate(policy[state]):\n",
    "                for  prob, next_state, reward, done in env.model[state][action]:\n",
    "                    state_value += action_prob * prob * (reward + discount_factor * value_fn[next_state])\n",
    "                delta = max(delta, np.abs(state_value - value_fn[state]))\n",
    "                value_fn[state] = state_value\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return value_fn\n",
    "    \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will learn about Policy Iteration in the subsequent modules, however we can observe that Policy Iteration is able to learn a policy that would take the Agent to the terminal state starting from any internal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "policy, v = policy_iteration(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 2]\n",
      " [0 0 0 2]\n",
      " [0 0 1 2]\n",
      " [0 1 1 0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also compute the Value Function for each state that corresponds to the number of steps required for the Agent to reach the terminal state since the reward is -1 for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function:\n",
      "Reshaped Grid Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Value Function:\")\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
